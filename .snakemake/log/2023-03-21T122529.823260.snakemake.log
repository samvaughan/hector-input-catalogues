Building DAG of jobs...
Using shell: /bin/bash
Provided cores: 1 (use --cores to define parallelism)
Rules claiming more threads will be scaled down.
Job stats:
job                        count    min threads    max threads
-----------------------  -------  -------------  -------------
all                            1              1              1
prepare_WAVES_catalogue        1              1              1
total                          2              1              1

Select jobs to execute...

[Tue Mar 21 12:25:30 2023]
Job 2: Preparing the WAVES catalogue
Reason: Missing output files: results/FinalInputCatalogues/WAVES_S_redshifts_final_input.parquet, results/FinalInputCatalogues/WAVES_N_redshifts_final_input.parquet

[Tue Mar 21 12:26:49 2023]
Finished job 2.
1 of 2 steps (50%) done
Select jobs to execute...

[Tue Mar 21 12:26:49 2023]
localrule all:
    input: results/Redshift_Cats/all_2dF_observations.parquet, results/FinalInputCatalogues/WAVES_N_redshifts_final_input.parquet, results/FinalInputCatalogues/WAVES_S_redshifts_final_input.parquet
    jobid: 0
    reason: Input files updated by another job: results/FinalInputCatalogues/WAVES_S_redshifts_final_input.parquet, results/FinalInputCatalogues/WAVES_N_redshifts_final_input.parquet
    resources: tmpdir=/var/folders/bd/ym7_px1s7_53zt92h0rfdb000000gp/T

[Tue Mar 21 12:26:49 2023]
Finished job 0.
2 of 2 steps (100%) done
Complete log: .snakemake/log/2023-03-21T122529.823260.snakemake.log
